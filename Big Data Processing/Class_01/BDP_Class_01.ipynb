{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef1024f-36cc-4fae-856f-23ac2dd50422",
   "metadata": {},
   "source": [
    "# Занятие №1. Параллельная обработка данных на чистом Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c9fed2-fb5b-4458-8ed6-929e88638927",
   "metadata": {},
   "source": [
    "На занятии мы воспроизвели на Python паттерны, которые Apache Spark использует «под капотом». Итоговая программа читает большой файл с датасетом, фильтрует нужные события, группирует их в пакеты и обрабатывает параллельно в нескольких процессах.\n",
    "\n",
    "**Такой подход решает две проблемы:**\n",
    "- файл с событиями может весить десятки гигабайт; если загрузить его целиком в список Python, программа упадёт с ошибкой `MemoryError` или начнёт использовать файл подкачки `swap`, что сильно замедлит работу;\n",
    "- интепретатор Python выполняет байткод только в одном потоке одновременно, даже если в компьютере, например, восемь ядер, обычная однопоточная программа нагрузит только одно из них, а остальные семь будут простаивать.\n",
    "\n",
    "Финальный вариант программы состоит из несколько логических блоков, по которым мы разделим наши действия.\n",
    "\n",
    "**Всего таких блоков восемь:**\n",
    "1. **Прописываем импорты и настраиваем окружение.** Прежде чем писать логику обработки, настроим окружение. Программа использует несколько модулей стандартной библиотеки:\n",
    "\n",
    "    - **functools** — содержит декораторы, которые позволяют работать с другими функциями. Из этого модуля нам понадобится декоратор `wraps`, который сохраняет метаданные функции, которую мы будем оборачивать;\n",
    "    - **json** — парсит JSON-строки в словари Python;\n",
    "    - **logging** — записывает сообщения о том, как выполняется программа; логирование поможет нам понять, что делает конкретный процесс;\n",
    "    - **time** — измеряет время, за которое выполняются функции;\n",
    "    - **collections.abc** — содержит абстрактные типы `Iterable` и `Iterator` для аннотаций, которые мы будем использовать;\n",
    "    - **concurrent.futures** — содержит высокоуровневый интерфейс, который позволяет запускать задачи параллельно в нескольких процессах или потоках;\n",
    "    - **pathlib** — предоставляет пути к файлам как к объектам, а не строкам.\n",
    "\n",
    "Константа `WORKDIR` хранит путь к директории со скриптом. От неё мы будем строить пути к данным.\n",
    "\n",
    "Логгер мы настраиваем так, чтобы каждое сообщение содержало имя процесса. Когда программа запустит несколько процессов параллельно, то сразу увидим, какой из них пишет в лог."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f549a972-a899-4af9-9604-e319de28317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фрагмент программы №1\n",
    "\n",
    "# Импортируем модуль, чтобы использовать декоратор wraps\n",
    "import functools\n",
    "\n",
    "# Импортируем модуль, чтобы парсить JSON-строки в словари\n",
    "import json\n",
    "\n",
    "# Импортируем модуль, чтобы логгер писал, какой процесс что делает\n",
    "import logging\n",
    "\n",
    "# Импортируем модуль, чтобы замерять, сколько секунд работает функция\n",
    "import time\n",
    "\n",
    "# Импортируем типы, чтобы аннотировать генераторы\n",
    "from collections.abc import Iterable, Iterator\n",
    "\n",
    "# Импортируем классы, чтобы запускать задачи в нескольких процессах параллельно\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# Импортируем класс, чтобы работать с путями как с объектами\n",
    "from pathlib import Path\n",
    "\n",
    "# Сохраняем путь к директории со скриптом, чтобы строить относительные пути к данным\n",
    "WORKDIR = Path(__file__).parent\n",
    "\n",
    "# Настраиваем логгер: каждое сообщение показывает имя процесса\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(processName)s %(message)s\")\n",
    "\n",
    "# Создаём логгер для этого модуля, чтобы писать сообщения из нашего кода\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b9d057-7de5-45e9-9ad2-39f7a76b6636",
   "metadata": {},
   "source": [
    "2. **Используем декоратор `slowlog`**\n",
    "    \n",
    "    **Декоратор** — это функция, которая принимает другую функцию и возвращает её модифицированную версию. Декоратор оборачивает функцию и выполняет дополнительный код до или после неё.\n",
    "\n",
    "    Декоратор `slowlog` измеряет, сколько времени выполняется функция, и пишет результат в лог. Параметр `threshold` задаёт порог в секундах. Если функция выполняется дольше, чем этот порог, то декоратор помечает её в логе как слишком медленную.\n",
    "\n",
    "    В программе используется **фабрика декораторов** — это функция, которая возвращает декоратор. Такая конструкция нужна, чтобы передать параметр `threshold`. \n",
    "\n",
    "    **Давайте разберём её структуру:**\n",
    "    1. **`slowlog(threshold)`** — это внешняя функция, которая принимает параметр `threshold` и возвращает декоратор `set_timer`.\n",
    "    2. **`set_timer(func)`** — это сам декоратор, который принимает функцию и возвращает обёртку `wrapper`.\n",
    "    3. **`wrapper(*args, **kwargs)`** — это обёртка, которая вызывает оригинальную функцию и логирует результат.\n",
    "    4. **`@functools.wraps(func)`** — это декоратор, который копирует в обёртку метаданные оригинальной функции. Например, имя и документацию. У каждой функции в Python есть атрибут `__name__`, который хранит её имя. Без декоратора `wraps` обёртка `wrapper` потеряет имя оргинальной функции, то есть атрибут `wrapper.__name__` вернёт строку \"wrapper\" вместо настоящего имени.\n",
    "    5. **`time.perf_counter()`** — это функция, которая использует монотонный счётчик процессора и возвращает текущее время в секундах время с максимальной точностью. Она лучше подходит, чтобы измерять короткие интервалы, чем функция `time.time()`, которая зависит от системных часов.\n",
    "    6. **`try/finally`** — это блок, который гарантирует, что время запишется в лог даже если функция выбросит исключение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc055a1-ecaa-4411-b863-b68c997d1f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фрагмент программы №2\n",
    "\n",
    "# Определяем фабрику декораторов, которая принимает порог в секундах\n",
    "def slowlog(threshold: float = 2.5):\n",
    "    \n",
    "    # Определяем сам декоратор, который принимает функцию\n",
    "    def set_timer(func):\n",
    "        \n",
    "        # Копируем имя и документацию оригинальной функции в обёртку\n",
    "        @functools.wraps(func)\n",
    "        \n",
    "        # Определяем обёртку, которая принимает любые аргументы оригинальной функции\n",
    "        def wrapper(*args, **kwargs):\n",
    "            \n",
    "            # Запоминаем время до вызова функции\n",
    "            start = time.perf_counter()\n",
    "            \n",
    "            # Блок try/finally гарантирует, что время запишется в лог, даже если функция выбросит исключение\n",
    "            try:\n",
    "                # Вызываем оригинальную функцию и сохраняем результат\n",
    "                result = func(*args, **kwargs)\n",
    "            finally:\n",
    "                # Вычисляем, сколько секунд выполнялась функция\n",
    "                delta = time.perf_counter() - start\n",
    "                \n",
    "                # Проверяем, превысило ли время порог\n",
    "                if delta > threshold:\n",
    "                    # Пишем в лог имя функции, время и пометку, что она слишком медленная\n",
    "                    logger.info(\"Finished %s in %.3f seconds (too slow)\", func.__name__, delta)\n",
    "                else:\n",
    "                    # Пишем в лог имя функции и время без пометки\n",
    "                    logger.info(\"Finished %s in %.3f seconds\", func.__name__, delta)\n",
    "            \n",
    "            # Возвращаем результат оригинальной функции\n",
    "            return result\n",
    "        \n",
    "        # Возвращаем обёртку вместо оригинальной функции\n",
    "        return wrapper\n",
    "    \n",
    "    # Возвращаем декоратор\n",
    "    return set_timer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1004c100-1e89-42cc-b430-c7b609b27b79",
   "metadata": {},
   "source": [
    "3. **Используем генератор `read_events`.**\n",
    "\n",
    "    **Генератор** — это функция, которая использует ключевое слово `yield` вместо `return`. Генератор не выполняет всю работу сразу, он возвращает объект-итератор, который выдаёт по одному значению при каждом обращении.\n",
    "\n",
    "    Функция `read_events` читает файл в формате **JSONL, или JSON Lines,** — это формат, в котором каждая строка файла является отдельным JSON-объектом. Такой формат удобно использовать для потоковой обработки, так как нам не нужно парсить весь файл целиком, чтобы получить первую запись.\n",
    "\n",
    "    Генератор открывает файл и проходит по нему построчно. Каждую строку он парсит в словарь и отдаёт через `yield`. Важно отметить, что пока внешний код не запросит следующее значение, генератор стоит на паузе и не читает следующую строку. Такой подход называется **ленивым вычислением**. Он позволяет сэкономить оперативную память, так как в каждый момент памяти находится только одна строка файла, а не весь файл целиком.\n",
    "\n",
    "    Аннотация `Iterator[dict]` означает, что функция возвращает итератор, который выдаёт словари."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be58b18-aabd-492b-ae80-25f501ba05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фрагмент программы №3\n",
    "\n",
    "# Определяем генератор, который принимает путь к файлу и возвращает итератор словарей\n",
    "def read_events(path: Path) -> Iterator[dict]:\n",
    "    \n",
    "    # Открываем файл на чтение в кодировке UTF-8\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        \n",
    "        # Проходим по файлу построчно\n",
    "        for line in f:\n",
    "            \n",
    "            # Парсим строку из JSON в словарь и отдаём через ключевое слово yield\n",
    "            yield json.loads(line)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9eb0c6-938d-404c-85d0-6a9454938412",
   "metadata": {},
   "source": [
    "4. **Используем генератор `filter_events`.**\n",
    "\n",
    "    Функция `filter_events` принимает итератор событий и возвращает новый итератор, который пропускает только события с нужным типом. Этот генератор тоже ленивый, так как он не создаёт список отфильтрованных событий в памяти. Вместо этого он берёт события из входного итератора по одному, проверяет условие и отдаёт подходящие дальше.\n",
    "\n",
    "    Когда мы соединяем `read_events` и `filter_events` в цепочку, то получаем **пайплайн** — это последовательность преобразований, где каждое звено обрабатывает данные и передаёт следующие. При этом ни одно звено не хранит данные целиком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd251ebf-22a3-4588-acfa-934737da80ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фрагмент программы №4\n",
    "\n",
    "# Определяем генератор, который принимает итератор событий и нужный тип события\n",
    "def filter_events(events: Iterator[dict], wanted_event_type: str) -> Iterator[dict]:\n",
    "    \n",
    "    # Проходим по событиям из входного итератора по одному\n",
    "    for event in events:\n",
    "        \n",
    "        # Проверяем, совпадает ли тип события с нужным\n",
    "        if event[\"event_type\"] == wanted_event_type:\n",
    "            \n",
    "            # Отдаём событие дальше, если тип совпал\n",
    "            yield event\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d8e5ed-6061-472f-94ef-57bb14f621dd",
   "metadata": {},
   "source": [
    "5. **Группируем потоки в пакеты.**\n",
    "\n",
    "    **Батч** — это пакет элементов фиксированного размера.\n",
    "\n",
    "    Когда программа передаёт данные в другой процесс, она сначала сериализует их, или превращает в байты. После чего отправляет, а процесс-получатель десереализует эти данные обратно. Эти операции занимают время. Если отправлять события по одному, то расходы на передачу данные перекроют время, которые мы получили благодаря параллельной обработки. Крупные пакеты выгоднее передавать, так как мы передаём один большой вместо тысяч маленьких.\n",
    "\n",
    "    Функция `batcher` принимает итератор и размер батча. Она складывает элементы в список. Как только список достигает нужного размера, `batcher` отдаёт его через `yield` и начинает собирать следующий. После того, как цикл завершиться, в конце файла элементов может быть меньше, чем размер батча. Для этого мы используем условие `if batch`, чтобы проверить, так ли это, и отдать остаток элементов последним неполным пакетом.\n",
    "\n",
    "    Аннтоация `Iterable[list[dict]]` говорит нам, что функция возвращает итерируемый объект, который выдаёт списки словарей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56843941-9460-42ef-ac28-8468a665a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фрагмент программы №5\n",
    "\n",
    "# Определяем генератор, который принимает итератор и размер батча\n",
    "def batcher(iterable: Iterable[dict], batch_size: int) -> Iterable[list[dict]]:\n",
    "    \n",
    "    # Создаём пустой список для накопления элементов\n",
    "    batch = []\n",
    "    \n",
    "    # Проходим по элементам входного итератора\n",
    "    for item in iterable:\n",
    "        \n",
    "        # Добавляем элемент в текущий батч\n",
    "        batch.append(item)\n",
    "        \n",
    "        # Проверяем, достиг ли батч нужного размера\n",
    "        if len(batch) == batch_size:\n",
    "            \n",
    "            # Отдаём заполненный батч через yield\n",
    "            yield batch\n",
    "            \n",
    "            # Создаём новый пустой список для следующего батча\n",
    "            batch = []\n",
    "    \n",
    "    # Проверяем, остались ли элементы после завершения цикла\n",
    "    if batch:\n",
    "        \n",
    "        # Отдаём оставшиеся элементы последним неполным батчем\n",
    "        yield batch\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eabc3a-a014-4d37-be86-30a05de4b931",
   "metadata": {},
   "source": [
    "6. **Обрабатываем батч и имитируем CPU-bound операцию.**\n",
    "\n",
    "    Функция `process_batch` имитирует тяжёлую вычислительную операцию. Когда мы вызываем `time.sleep(3)`, мы заставляем процесс ждать три секунды.\n",
    "\n",
    "    Декоратор `@slowlog(5)` измеряет время, за сколько выполнится программа. Порог в пять секунд означает, что если функция выполняется дольше, лог пометит, что она медленная.\n",
    "\n",
    "    Функция `process_batch` возвращает количество обработанных событий. Функция `main` суммирует эти значения, чтобы подсчитать общее число событий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8010cd6d-5695-43da-95f0-4a8376514990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фрагмент программы №6\n",
    "\n",
    "# Оборачиваем функцию декоратором slowlog с порогом 5 секунд\n",
    "@slowlog(5)\n",
    "\n",
    "# Определяем функцию, которая принимает батч и возвращает количество событий в нём\n",
    "def process_batch(batch: list[dict]) -> int:\n",
    "    \n",
    "    # Имитируем тяжёлую вычислительную операцию, которая длится 3 секунды\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Возвращаем количество обработанных событий\n",
    "    return len(batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3008f0-5d29-489e-bcc9-897752b0e1f7",
   "metadata": {},
   "source": [
    "7. **Используем оркестрацию, чтобы связать все компоненты вместе.**\n",
    "\n",
    "    Функция `main` собирает все компоненты в единый пайплайн и запускает параллельную обработку батчей.\n",
    "\n",
    "    1. **Строим пайплайн, который состоит из цепочки генераторов:**\n",
    "        1. `read_events(filename)` — возвращает итератор, который читает файл построчно.\n",
    "        2. `filter_events(events, 'purchase')` — оборачивает предыдущий итератор и фильтрует события.\n",
    "        3. `batcher(filtered_events, batch_size=50_000)` — оборачивает фильтр и группирует события в батчи по 50 000 штук. На этом этапе программа ещё не прочитала ни одну строку файла.\n",
    "\n",
    "            Генераторы начнут работать только тогда, когда кто-то запросит у них данные, потому что они ленивые.\n",
    "            \n",
    "    2. **Инициализируем параллельное выполнение через `ProcessPoolExecutor`.**\n",
    "\n",
    "        **`ProcessPoolExecutor`** — это класс из модуля `concurrent.futures`. Он создаёт пул процессов, то есть набор заранее запущенных процессов, которые ждут задачи. Когда мы отправляем задачу в пул, один из свободных процессов выполняет её. По умолчанию количество процессов в пуле равно числу ядер процессора.\n",
    "\n",
    "        Мы используем процессы, а не потоки, потому что в Python есть **GIL, или Global Interpreter Lock, то есть глобальная блокировка интерпретатора.** GIL не позволяет нескольким потокам выполнять байт-код одновременно. Потоки помогают только для I/O-bound-задачами, которые связаны, например, с сетью, где поток большую часть времени просто ждёт. Для CPU-bound задач, в которых много вычислений, нужны отдельные процессы, каждый из которых имеет свой интерпретатор и GIL.\n",
    "\n",
    "        Менеджер контекста `with ProcessPoolExecutor() as executor` автоматически завершит все процессы при выходе из блока.\n",
    "\n",
    "    3. **Отправляем задачи и собираем результат**.\n",
    "\n",
    "        Метод `executor.submit(process_batch, batch)` отправляет задачу в пул и сразу возвращает объект `Future`, который указывает на то, что результат обязательно будет. Эта задача выполянется в фоне, а основной код продолжает работать.\n",
    "\n",
    "        Генератор списка `[executor.submit(process_batch, batch) for batch in batches]` итерируется по генератору батчей. В этот момент генераторы начинают читать файл, фильтровать и группировать данные. Каждый готовый батч сразу уходит в пул на обработку.\n",
    "\n",
    "        Функция `as_completed(futures)` возвращает итератор, который выдаёт объекты по мере того, как они завершаются. Это значит, что какой процесс первый закончит работать, то первым и отдаст результат. То есть не нужно ждать, пока завершатся все задачи, чтобы начать обрабатывать результаты.\n",
    "\n",
    "        Метод `future.result()` блокирует выполнение кода, пока задача не завершится, и возвращает результат. Если задача выбросила исключение, `result()` его пробросит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407d98d7-8203-4994-aecb-8db9783b9253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фрагмент программы №7\n",
    "\n",
    "# Оборачиваем функцию декоратором slowlog с порогом 20 секунд\n",
    "@slowlog(20)\n",
    "\n",
    "# Определяем главную функцию, которая связывает все компоненты\n",
    "def main():\n",
    "    \n",
    "    # Строим путь к файлу с событиями относительно директории скрипта\n",
    "    filename = WORKDIR / \"data\" / \"events.jsonl\"\n",
    "    \n",
    "    # Создаём генератор, который читает файл построчно\n",
    "    events = read_events(filename)\n",
    "    \n",
    "    # Оборачиваем генератор фильтром, который пропускает только события с типом \"purchase\"\n",
    "    filtered_events = filter_events(events, \"purchase\")\n",
    "    \n",
    "    # Оборачиваем фильтр генератором, который группирует события в батчи по 50 000 штук\n",
    "    batches = batcher(filtered_events, batch_size=50_000)\n",
    "    \n",
    "    # Инициализируем счётчик обработанных событий\n",
    "    total = 0\n",
    "    \n",
    "    # Создаём пул процессов, который автоматически завершится при выходе из блока\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        \n",
    "        # Отправляем каждый батч в пул на обработку и собираем объекты Future в список\n",
    "        futures = [executor.submit(process_batch, batch) for batch in batches]\n",
    "        \n",
    "        # Получаем результаты по мере завершения задач\n",
    "        for future in as_completed(futures):\n",
    "            \n",
    "            # Прибавляем количество обработанных событий к общему счётчику\n",
    "            total += future.result()\n",
    "    \n",
    "    # Пишем в лог общее количество обработанных событий\n",
    "    logger.info(\"Total events: %d\", total)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a5901-132a-4abe-80cb-e9829adeefca",
   "metadata": {},
   "source": [
    "8. **Защищаемся от повторного импорта.**\n",
    "\n",
    "    Условие `if __name__ == \"__main__\"` проверяет, что файл запущен напрямую, а не импортирован как модуль.\n",
    "\n",
    "    Когда мы работаем с `ProcessPoolExecutor` это особенно важно. Когда пул создаёт новый процесс, он импортирует модуль заново. Если бы мы не использовали `if __name__ == \"__main__\"`, то каждый дочерний процесс пытался бы создать свой пул процессов, который создал бы ещё процессы, и так до бесконечности. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085bc79-172a-4c9b-8951-b3e57306e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фрагмент программы №8\n",
    "\n",
    "# Проверяем, запущен ли файл напрямую, а не импортирован как модуль\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Вызываем главную функцию\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
